* DONE test heteroscedastic
distribution of all residue is gaussian and for weighed residues? or just normalized by the  sqrt(y_i) value

** Residual Distribution Analysis (532 points from L1, L2, L4, 140220)
| Method             |   Std | Shapiro p | Conclusion            |
|--------------------+-------+-----------+-----------------------|
| lm_physics         | 2.807 |     0.000 | errors underestimated |
| outlier2_uniform   | 1.036 |     0.778 | ✓ BEST ~N(0,1)        |
| outlier2_shotnoise | 1.127 |     0.265 | ✓ Good, ~N(0,1)       |
| lm_reweighted      | 0.817 |     0.000 | errors overestimated  |

** Coverage Analysis (38 control wells with known pKa from L1+L2+L4+140220)
| Method               | K_err | Bias   | RMSE  | Coverage |
|----------------------+-------+--------+-------+----------|
| lm_robust            | 0.161 | +0.036 | 0.135 | 89.5% ✓  |
| lm_physics           | 0.119 | +0.059 | 0.162 | 84.2%    |
| odr_physics          | 0.097 | +0.011 | 0.132 | 81.6%    |
| pymc_physics_shared  | 0.109 | +0.060 | 0.171 | 81.6%    |
| outlier2_uniform     | 0.052 | +0.009 | 0.128 | 50.0% ✗  |
| outlier2_shotnoise   | 0.051 | +0.011 | 0.119 | 47.4% ✗  |

** Synthetic Data Validation (calibrated to match real data)
Parameters: n_points=7, buffer_sd=40, S0_y1=600, S0_y2=500
| Method           | Synth K_err | Real K_err | Synth Cov | Real Cov |
|------------------+-------------+------------+-----------+----------|
| lm_robust        |       0.140 |      0.161 |     89.0% |   89.5%  | ✓ Match
| lm_physics       |       0.121 |      0.119 |     89.0% |   84.2%  | ✓ Match
| odr_physics      |       0.122 |      0.097 |     86.0% |   81.6%  | ~Match
| outlier2_uniform |       0.112 |      0.052 |     87.0% |   50.0%  | Mismatch

** Key Finding: Heteroscedasticity Paradox
outlier2_uniform produces residuals closest to N(0,1) but has POOR coverage (50%)
because uncertainties are underestimated in absolute scale.
Synthetic data shows higher coverage (87%) → real data has additional systematic effects.

** Recommendation
- Use lm_robust (89.5% coverage) or odr_physics (81.6% coverage, lowest bias)
- Avoid residual-based methods for final uncertainty quantification
- Synthetic data generator calibrated to match real data (buffer_sd=40)

Plot: docs/residuals_all_methods.png
Results: comprehensive_fitting_results_with_L1.csv
* DONE Comprehensive comparison of fitting methods (2024-11-30)
benchmarks/compare_fitting_methods.py - Statistical comparison with synthetic & real data

** Generated Graphics (committed before function removal)
- benchmarks/synthetic_data_examples.png - Example synthetic data with/without outliers
- benchmarks/coverage_comparison.png - Coverage bar chart across methods
- benchmarks/bias_comparison.png - Bias bar chart across methods
- benchmarks/coverage_vs_bias.png - Coverage vs bias scatter
- benchmarks/uncertainty_calibration.png - K_err vs actual coverage
- benchmarks/bias_rmse_comparison.png - Bias vs RMSE scatter plots

** Final API (simplified after benchmarking)

*** fit_binding_glob(ds, robust=False, scale_covar=True)
- Default: standard least-squares with covariance scaling
- robust=True: Huber loss for outlier resistance (BEST bias with outliers)
- scale_covar=True: scales K_err by sqrt(reduced_chi_sq) when > 1

*** fit_binding_glob_robust(ds, error_model="uniform")
- Iterative error reweighting + Huber loss
- Higher coverage on real data but worse bias with outliers
- Use when error estimates are unreliable

** Results Summary
| Method | Clean Bias | Clean Cov | Outlier Bias | Outlier Cov |
|--------|------------|-----------|--------------|-------------|
| lm_standard | -0.002 | 94% | -0.059 | 100% |
| lm_robust | +0.024 | 62% | -0.018 | 80% |
| lm_robust+scale | +0.024 | 67% | -0.018 | 80% |
| fit_glob_robust | +0.041 | 82% | -0.097 | 84% |

** Final Benchmark Results (2024-11-30) - Realistic Synthetic Data
Synthetic data calibrated to match real data characteristics:
- y1: inverted curve, range ~460, 6% relative error (FRET acceptor)
- y2: normal curve, range ~900, 3% relative error (FRET donor)
- Outliers: 4σ drop in y1 at low pH (mimics real data pattern)

| Method | Clean Bias | Outlier Bias | Clean RMSE | Outlier RMSE |
|--------|------------|--------------|------------|--------------|
| outlier2 | -0.0001 | **0.007** | 0.048 | **0.049** |
| lm_standard | -0.003 | 0.032 | **0.041** | 0.053 |
| lm_robust | -0.002 | 0.010 | 0.049 | 0.052 |

Real data (38 wells):
| Method | Mean K | Std K |
|--------|--------|-------|
| outlier2 | 6.69 | 0.36 |
| lm_standard | 6.76 | 0.35 |
| lm_robust | 6.72 | **0.33** |

*** Conclusions:
1. outlier2 is BEST for outlier handling (4x lower bias than lm_standard)
2. lm_standard has best clean RMSE but worst outlier robustness
3. lm_robust is good compromise - second-best outlier bias, lowest variance on real data

** Outlier Magnitude Sensitivity Analysis (2024-12-01)
benchmarks/outlier_magnitude_comparison.png

Tested 2 outliers at low pH in y1 channel (DROP pattern like real data)
50 trials per outlier magnitude, scale_covar=True for all methods

| Sigma | Std Bias | Rob Bias | Std RMSE | Rob RMSE | Std Cov | Rob Cov |
|-------|----------|----------|----------|----------|---------|---------|
|     0 |   0.0053 |  -0.0006 |   0.0863 |   0.0933 |   96.0% |   94.3% |
|     3 |   0.0076 |   0.0162 |   0.0885 |   0.0933 |   98.0% |   97.2% |
|     5 |   0.0308 |   0.0215 |   0.0971 |   0.1051 |  100.0% |  100.0% |
|     8 |   0.1043 |   0.0437 |   0.1467 |   0.1337 |  100.0% |  100.0% |
|    10 |   0.1875 |   0.0665 |   0.2206 |   0.1440 |  100.0% |  100.0% |

*** Conclusions:
1. Robust (Huber) reduces bias by ~65% for severe outliers (10σ)
2. RMSE crossover at ~8σ - Robust wins for large outliers
3. Both maintain ~95-100% coverage thanks to scale_covar=True
4. For typical outliers (3-5σ): Standard WLS is sufficient
5. For severe outliers (>5σ): Use robust=True

** Residual Normality Analysis (2024-12-01)
benchmarks/residual_normality.png

Checks if standardized residuals follow Gaussian distribution (critical for valid CI):

Real Data (38 controls):
| Method | Shapiro p-value | Excess Kurtosis | Normality |
|--------|-----------------|-----------------|-----------|
| lm_standard | 0.003 | 1.2 | Rejected |
| lm_robust | 0.008 | 0.9 | Rejected |
| outlier2 | 0.015 | 0.8 | Rejected |

Synthetic with Outliers (N=100):
| Method | Shapiro p-value | Excess Kurtosis | Normality |
|--------|-----------------|-----------------|-----------|
| lm_standard | <0.001 | 2.5 | Rejected |
| lm_robust | 0.02 | 1.1 | Rejected |
| outlier2 | 0.08 | 0.5 | Marginal |

*** Key insight:
All methods show some deviation from normality on real data.
outlier2's Bayesian approach produces the most Gaussian residuals with outliers.

** Final Recommendation (2024-12-01)
- fit_binding_glob(ds, scale_covar=True) - DEFAULT for most use cases
  * Proper heteroscedastic error weighting naturally downweights outliers
  * When y1_err >> y2_err (typical: 100 vs 40), outliers in y1 have ~6x less influence
  * No information loss from outlier masking

- outlier2(ds, error_model="uniform") - When explicit outlier identification needed
  * Best residual normality with outliers (Shapiro p=0.08)
  * Most Gaussian residuals → most reliable uncertainty estimates
  * Use when downstream analysis requires Gaussian assumptions

** Action: REMOVE deprecated functions
- fit_binding_reweight - superseded by outlier2()
- fit_binding_glob_robust - standard WLS better with heteroscedastic errors
- fit_binding_glob_recursive_outlier - poor coverage, complex

** Keep (final API)
- fit_binding_glob(ds, scale_covar=True) - standard least-squares (PRIMARY)
- fit_binding_glob_recursive(ds) - conservative uncertainty estimates
- outlier2(ds, error_model="uniform") - Bayesian with explicit outlier detection

* for manuscript
   Key observations:

     - Coverage is similar (~85-88%) regardless of whether data was generated with physics or uniform errors
     - lm_physics, lm_robust, and odr_physics all achieve ~86-88% coverage
     - outlier2 methods achieve ~81-83% coverage (slightly lower but still reasonable)
     - lm_reweighted is consistently poor (~50% coverage) - overconfident
     - Coverage doesn't vary much with pKa - method performance is stable across the pKa range

** Synthetic Comparison: Data Error Model × Fit Error Model (N=30, pKa=6.0-8.0)
Results from benchmarks/compare_error_models.py

| Data Error | Fit Error | Method       | K_err | bias   | RMSE  | Coverage |
|------------+-----------+--------------+-------+--------+-------+----------|
| PHYSICS    | PHYSICS   | lm_physics   | 0.139 | +0.018 | 0.148 | 87.3%    |
|            |           | lm_robust    | 0.157 | +0.014 | 0.167 | 86.0%    |
|            |           | odr          | 0.149 | +0.007 | 0.178 | 86.0%    |
|            |           | outlier2     | 0.124 | +0.021 | 0.166 | 81.3%    |
| PHYSICS    | UNIFORM   | lm_robust    | 0.160 | +0.021 | 0.163 | 88.0%    |
|            |           | lm           | 0.140 | +0.020 | 0.148 | 88.0%    |
|            |           | odr          | 0.147 | +0.008 | 0.173 | 86.7%    |
|            |           | outlier2     | 0.124 | +0.023 | 0.167 | 81.3%    |
| UNIFORM    | PHYSICS   | lm_physics   | 0.139 | +0.017 | 0.149 | 88.0%    |
|            |           | lm_robust    | 0.158 | +0.023 | 0.162 | 86.7%    |
|            |           | odr          | 0.152 | +0.006 | 0.182 | 85.3%    |
|            |           | outlier2     | 0.124 | +0.020 | 0.167 | 81.3%    |
| UNIFORM    | UNIFORM   | lm           | 0.140 | +0.018 | 0.149 | 88.7%    |
|            |           | lm_robust    | 0.159 | +0.025 | 0.170 | 86.0%    |
|            |           | odr          | 0.149 | +0.007 | 0.177 | 86.0%    |
|            |           | outlier2     | 0.124 | +0.021 | 0.168 | 81.3%    |

*** Key findings:
1. Coverage is remarkably stable (~85-88%) across all error model combinations
2. Matching data and fit error models doesn't significantly improve coverage
3. outlier2 consistently achieves ~81% coverage regardless of error model
4. Best coverage (88.7%): Uniform data + Uniform fit + lm method
5. Real data's lower coverage for outlier2 (50%) suggests additional systematic
   effects not captured in synthetic simulation (reference pKa uncertainty, etc.)

* TODO Next Steps
+1. Analyze real experimental data characteristics
+1. Refine synthetic data generation to match real data
+1. Run comprehensive benchmarks
+1. Validate recommendations
+1. Implement cleanup based on validated results

* optimized
#+begin_src jupyter-python
  minimizer = Minimizer(
      _binding_1site_residuals, params, fcn_args=(ds,), scale_covar=True
  )

  # Use Huber loss for robustness to outliers
  huber_result = minimizer.minimize(method="least_squares", loss="huber")

  ...

      # Update weights based on residuals (IRLS step)
      if iteration < irls_iterations - 1:  # Don't update weights on last iteration
          start_idx = 0
          for da in current_ds.values():
              end_idx = start_idx + len(da.y)
              residuals = irls_result.residual[start_idx:end_idx]

              # Conservative weight update: inverse of residual magnitude with damping
              old_errors = da.y_errc[da.mask].copy()
              new_errors = np.maximum(
                  np.abs(residuals), 1e-6
              )  # Prevent division by zero

              # Apply damping to prevent instability
              damped_errors = (
                  1 - irls_damping
              ) * old_errors + irls_damping * new_errors
              da.y_errc[da.mask] = damped_errors

              start_idx = end_idx

    #+end_src

* weighting
prtecan builds each DataArray in Titration._create_data_array (src/clophfit/prtecan/prtecan.py) with y_errc = sqrt(signal + bg_err[label]^2) when buffer SEM exists (from Buffer
   fits/means), otherwise sqrt(signal) from the raw intensities. If no bg_err is provided, _create_ds calls weight_da (src/clophfit/fitting/core.py) to prefit each label with a 1-site model
   and replace y_err by the residual SEM (fallback = ones), while _create_global_ds calls weight_multi_ds_titration to do the same across labels and assigns any failed label a fallback error
   = (max successful y_err)×10. The downstream fitter fit_binding_glob uses residuals weighted by 1 / y_err, and outlier2 can optionally reweight uniformly or by shot-noise after a robust
   pass.

* simulation
1. 1 label s0 s1 fissi no drop
2. 1 lab
