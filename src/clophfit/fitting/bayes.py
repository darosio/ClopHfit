"""Bayesian (PyMC) fitting utilities and pipelines."""

from __future__ import annotations

import copy
import typing
from typing import Literal, TypedDict

import arviz as az
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pymc as pm  # type: ignore[import-untyped]
from lmfit import Parameters  # type: ignore[import-untyped]
from matplotlib import figure
from pydantic import BaseModel, ConfigDict
from pymc import math as pm_math
from pytensor.tensor import as_tensor_variable
from scipy import optimize

from clophfit.fitting.models import binding_1site
from clophfit.fitting.plotting import PlotParameters, plot_fit

from .core import N_BOOT  # local to avoid circular import
from .data_structures import DataArray, Dataset, FitResult, MiniT, _Result

if typing.TYPE_CHECKING:
    from clophfit.clophfit_types import ArrayF, FloatFunc
    from clophfit.prtecan import PlateScheme


def create_x_true(
    xc: ArrayF, x_errc: ArrayF, n_xerr: float, lower_nsd: float = 2.5
) -> ArrayF | pm.Deterministic:
    """Create latent variables for x-values with uncertainty."""
    if n_xerr > 0 and np.any(x_errc > 0):
        x_errc_scaled = x_errc * n_xerr
        xd = -np.diff(xc)
        xd_err = np.sqrt(x_errc_scaled[:-1] ** 2 + x_errc_scaled[1:] ** 2)
        lower = xd.min() - lower_nsd * xd_err[np.argmin(xd)]
        # MAYBE: It was logger.info(f"min pH distance: {lower}")
        x_diff = pm.TruncatedNormal(
            "x_diff", mu=xd, sigma=xd_err, lower=lower, shape=len(xc) - 1
        )
        x_start = pm.Normal("x_start", mu=xc[0], sigma=x_errc_scaled[0])
        x_cumsum = pm.math.cumsum(x_diff)
        return pm.Deterministic(
            "x_true", pm.math.concatenate([[x_start], x_start - x_cumsum])
        )
    return xc


def create_parameter_priors(
    params: Parameters, n_sd: float, key: str = "", ctr_name: str = ""
) -> dict[str, pm.Distribution]:
    """Create parameter priors."""
    priors: dict[str, pm.Distribution] = {}

    def param_name(p_name: str) -> str:
        return f"{p_name}_{key}" if key else p_name

    for name, p in params.items():
        sigma = max(p.stderr * n_sd, 1e-3) if p.stderr else 1e-3
        if ctr_name and name == "K":  # shared K comes from control priors
            continue
        priors[param_name(name)] = pm.Normal(param_name(name), mu=p.value, sigma=sigma)
    return priors


# TODO:
# 🧪 Test posterior integrity (e.g., credible intervals contain true Kd)
# 🧱 Replace repetitive for lbl in ds.items() logic using helper functions
# 🔁 Use pm.MutableData (in newer PyMC versions) to avoid model recompilation
def rename_keys(data: dict[str, typing.Any]) -> dict[str, typing.Any]:
    """Rename dictionary keys coming from multi-trace into base names."""
    renamed_dict: dict[str, typing.Any] = {}
    for key, value in data.items():
        if key.startswith("K_"):
            new_key = "K"
        elif key.rfind("_") > 1:
            new_key = key[: key.rfind("_")]
        else:
            new_key = key
        renamed_dict[new_key] = value
    return renamed_dict


def process_trace(
    trace: az.InferenceData, p_names: typing.KeysView[str], ds: Dataset, n_xerr: float
) -> FitResult[az.InferenceData]:
    """Process the trace to extract parameter estimates and update datasets.

    Parameters
    ----------
    trace : az.InferenceData
        The posterior samples from PyMC sampling.
    p_names: typing.KeysView[str]
        Parameter names.
    ds : Dataset
        The dataset containing titration data.
    n_xerr : float
        Scaling factor for `x_errc`.

    Returns
    -------
    FitResult[az.InferenceData]
        The updated fit result with extracted parameter values and datasets.
    """
    # Extract summary statistics for parameters
    rdf = az.summary(trace)
    rpars = Parameters()
    for name, row in rdf.iterrows():
        if name in p_names:
            rpars.add(name, value=row["mean"], min=row["hdi_3%"], max=row["hdi_97%"])
            rpars[name].stderr = row["sd"]
            rpars[name].init_value = row.get("r_hat", np.nan)
    # x_true and x_errc
    nxc: list[float] = []
    nx_errc: list[float] = []
    for name, row in rdf.iterrows():
        if isinstance(name, str) and name.startswith("x_true"):
            nxc.append(row["mean"])
            nx_errc.append(row["sd"])
    for da in ds.values():
        da.xc = np.array(nxc)  # Update x_true values in the dataset
        da.x_errc = (
            np.array(nx_errc) * n_xerr
        )  # Scale the errors FIXME: n_xerr not needed
    # Scale y_errc if present
    try:
        mag = float(rdf.loc["ye_mag", "mean"])  # type: ignore[arg-type, index]
    except Exception:  # noqa: BLE001
        mag = 1.0
    for da in ds.values():
        da.y_errc *= mag  # Scale y errors by the magnitude
    # Create figure
    fig = figure.Figure()
    ax = fig.add_subplot(111)
    # FIXME: multi need this renaming quite surely
    rename_keys(rpars)
    plot_fit(ax, ds, rpars, nboot=N_BOOT, pp=PlotParameters(ds.is_ph))

    # Return the fit result
    return FitResult(fig, _Result(rpars), trace, ds)


def extract_fit(
    key: str, ctr: str, trace_df: pd.DataFrame, ds: Dataset
) -> FitResult[az.InferenceData]:
    """Compute individual dataset fit from a multi-well trace summary."""
    rpars = Parameters()
    rdf = trace_df[trace_df.index.str.endswith(key)]
    for name, row in rdf.iterrows():
        extracted_name = str(name).replace(f"_{key}", "")
        rpars.add(
            extracted_name, value=row["mean"], min=row["hdi_3%"], max=row["hdi_97%"]
        )
        rpars[extracted_name].stderr = row["sd"]
        rpars[extracted_name].init_value = row.get("r_hat", np.nan)
    if ctr:
        rdf = trace_df[trace_df.index.str.endswith(ctr)]
        for name, row in rdf.iterrows():
            extracted_name = str(name).replace(f"_{ctr}", "")
            rpars.add(
                extracted_name, value=row["mean"], min=row["hdi_3%"], max=row["hdi_97%"]
            )
            rpars[extracted_name].stderr = row["sd"]
            rpars[extracted_name].init_value = row.get("r_hat", np.nan)
    nxc: list[float] = []
    nx_errc: list[float] = []
    for name, row in trace_df.iterrows():
        if isinstance(name, str) and name.startswith("x_true"):
            nxc.append(row["mean"])
            nx_errc.append(row["sd"])
    for da in ds.values():
        da.xc = np.array(nxc)
        da.x_errc = np.array(nx_errc)
    # Create figure
    fig = figure.Figure()
    ax = fig.add_subplot(111)
    plot_fit(ax, ds, rpars, nboot=N_BOOT, pp=PlotParameters(ds.is_ph))
    return FitResult(fig, _Result(rpars), az.InferenceData(), ds)


def x_true_from_trace_df(trace_df: pd.DataFrame) -> DataArray:
    """Extract x_true from an ArviZ summary DataFrame."""
    nxc: list[float] = []
    nx_errc: list[float] = []
    for name, row in trace_df.iterrows():
        if isinstance(name, str) and name.startswith("x_true"):
            nxc.append(row["mean"])
            nx_errc.append(row["sd"])
    return DataArray(xc=np.array(nxc), yc=np.ones_like(nxc), x_errc=np.array(nx_errc))


def fit_binding_pymc(
    fr: FitResult[MiniT],
    n_sd: float = 10.0,
    n_xerr: float = 1.0,
    ye_scaling: float = 1.0,
    n_samples: int = 2000,
) -> FitResult[az.InferenceData]:
    """Analyze multi-label titration datasets using PyMC (single model)."""
    if fr.result is None or fr.dataset is None:
        return FitResult()
    params = fr.result.params
    ds = copy.deepcopy(fr.dataset)
    xc = next(iter(ds.values())).xc  # # TODO: move up out
    x_errc = next(iter(ds.values())).x_errc
    with pm.Model():
        pars = create_parameter_priors(params, n_sd)
        x_true = create_x_true(xc, x_errc, n_xerr)
        # Add likelihoods for each dataset
        ye_mag = pm.HalfNormal("ye_mag", sigma=ye_scaling)
        for lbl, da in ds.items():
            y_model = binding_1site(
                x_true, pars["K"], pars[f"S0_{lbl}"], pars[f"S1_{lbl}"], ds.is_ph
            )
            pm.Normal(
                f"y_likelihood_{lbl}",
                mu=y_model[da.mask],
                sigma=ye_mag * da.y_err,
                observed=da.y,
            )
        # Inference
        tune = n_samples // 2
        trace = pm.sample(
            n_samples, tune=tune, target_accept=0.9, cores=4, return_inferencedata=True
        )
    return process_trace(trace, params.keys(), ds, n_xerr)


def fit_binding_pymc2(
    fr: FitResult[MiniT],
    n_sd: float = 10.0,
    n_xerr: float = 1.0,
    n_samples: int = 2000,
) -> FitResult[az.InferenceData]:
    """Analyze multi-label titration datasets using PyMC with separate ye_mag per label."""
    if fr.result is None or fr.dataset is None:
        return FitResult()
    params = fr.result.params
    ds = copy.deepcopy(fr.dataset)
    xc = next(iter(ds.values())).xc  # # TODO: move up out
    x_errc = next(iter(ds.values())).x_errc
    with pm.Model():
        pars = create_parameter_priors(params, n_sd)
        x_true = create_x_true(xc, x_errc, n_xerr)
        # Add likelihoods for each dataset
        ye_mag: dict[str, pm.Distribution] = {}
        for lbl in ds:
            ye_mag[lbl] = pm.HalfNormal(f"ye_mag_{lbl}", sigma=10.0)  # TODO: 100
        for lbl, da in ds.items():
            y_model = binding_1site(
                x_true, pars["K"], pars[f"S0_{lbl}"], pars[f"S1_{lbl}"], ds.is_ph
            )
            pm.Normal(
                f"y_likelihood_{lbl}",
                mu=y_model[da.mask],
                sigma=ye_mag[lbl] * np.ones_like(da.y_err),
                observed=da.y,
            )
        # Inference
        tune = n_samples // 2
        trace = pm.sample(
            n_samples, tune=tune, target_accept=0.9, cores=4, return_inferencedata=True
        )
    return process_trace(trace, params.keys(), ds, n_xerr)


def fit_binding_pymc_compare(  # noqa: PLR0913
    fr: FitResult[MiniT],
    buffer_sd: dict[str, float],
    learn_separate_y_mag: bool = False,
    n_sd: float = 10.0,
    n_xerr: float = 1.0,
    n_samples: int = 2000,
) -> az.InferenceData:
    """
    Fits a Bayesian binding model with two different noise models for comparison.

    Parameters
    ----------
    fr : FitResult[MiniT]
        The fit result from a previous run, providing initial parameters and dataset.
    buffer_sd : dict[str, float]
        bg_err
    learn_separate_y_mag : bool
        If True, learns a unique noise scaling factor for each dataset label.
        If False, learns a single scaling factor for all pre-weighted data.
    n_sd : float
        Prior width for parameters in create_parameter_priors.
    n_xerr : float
        Scaling factor for x_errc in create_x_true.
    n_samples : int
        Number of MCMC samples to draw.

    Returns
    -------
    az.InferenceData
        The posterior samples from PyMC for the specified noise model.
    """
    """
    if fr.result is None or fr.dataset is None:
        msg = "Input FitResult object must contain a result and a dataset."
        raise ValueError(msg)
    """
    if fr.result:
        params = fr.result.params
    if fr.dataset:
        ds = copy.deepcopy(fr.dataset)

    # Use the first dataset's x values. Assumes all datasets have same x points.
    xc = next(iter(ds.values())).xc
    x_errc = next(iter(ds.values())).x_errc

    with pm.Model():
        # Create priors for all parameters (K, S0_y1, S1_y1, etc.)
        pars = create_parameter_priors(params, n_sd)
        # Model the x-values with their uncertainties
        x_true = create_x_true(xc, x_errc, n_xerr)
        # ---------------------------------------------------------------------
        # Core conditional logic for the noise model

        if learn_separate_y_mag:
            # Model 1: Learn a unique noise scaling factor for each label
            # This is robust when you don't trust the initial y_err values
            ye_mag: dict[str | int, float] = {}
            true_buffer = {}
            for lbl, da in ds.items():
                ye_mag[lbl] = pm.HalfNormal(f"ye_mag_{lbl}", sigma=da.y_err.mean())
                y_model = binding_1site(
                    x_true, pars["K"], pars[f"S0_{lbl}"], pars[f"S1_{lbl}"], ds.is_ph
                )
                true_buffer[lbl] = pm.Normal(
                    f"true_buffer_{lbl}", mu=0, sigma=da.y_err.mean()
                )
                sigma = 10 * pm.math.sqrt(
                    (ye_mag[lbl] * np.ones_like(da.y_err)) ** 2 + buffer_sd[lbl] ** 2
                    # Alternatively use: ye_mag[lbl] ** 2 * da.y + buffer_sd[lbl] ** 2
                )

                pm.Normal(
                    f"y_likelihood_{lbl}",
                    mu=y_model[da.mask] + true_buffer[lbl],
                    sigma=sigma,
                    # Noise is learned from scratch and shot noise model
                    # Alternatively use: * np.ones_like(da.y_err),# Noise is learned from scratch
                    observed=da.y,
                )
        else:
            # Model 2: Learn a single noise scaling factor for all data
            # This is appropriate when you trust the relative y_err values
            ye_mag0 = pm.HalfNormal("ye_mag", sigma=10.0)
            for lbl, da in ds.items():
                y_model = binding_1site(
                    x_true, pars["K"], pars[f"S0_{lbl}"], pars[f"S1_{lbl}"], ds.is_ph
                )
                pm.Normal(
                    f"y_likelihood_{lbl}",
                    mu=y_model[da.mask],
                    sigma=ye_mag0 * da.y_err,  # Apply a single scaling factor
                    # Alternatively use:  sigma=da.y_err,  # Apply a single scaling factor
                    observed=da.y,
                )
        # ---------------------------------------------------------------------
        # Run MCMC sampling
        trace: az.InferenceData = pm.sample(
            n_samples, cores=4, return_inferencedata=True, target_accept=0.9
        )
    return trace


def closest_point_on_curve(f: FloatFunc, x_obs: float, y_obs: float) -> float:
    """Find the closest point on the model curve."""

    def objective(x_prime: float) -> float:
        return (x_obs - x_prime) ** 2 + (y_obs - f(x_prime)) ** 2

    result = optimize.minimize_scalar(objective)
    return float(result.x)


def fit_binding_pymc_odr(
    fr: FitResult[MiniT],
    n_sd: float = 10.0,
    xe_scaling: float = 1.0,
    ye_scaling: float = 10.0,
    n_samples: int = 2000,
) -> az.InferenceData | pm.backends.base.MultiTrace:
    """Bayesian ODR-like modeling of x and y errors."""
    if fr.result is None or fr.dataset is None:
        return az.InferenceData()  # FitResult()
    params = fr.result.params
    ds = copy.deepcopy(fr.dataset)
    xc = next(iter(ds.values())).xc
    x_errc = next(iter(ds.values())).x_errc
    with pm.Model() as _:
        pars = create_parameter_priors(params, n_sd)
        # Add likelihoods for each dataset
        ye_mag = pm.HalfNormal("ye_mag", sigma=ye_scaling)
        xe_mag = pm.HalfNormal("xe_mag", sigma=xe_scaling)

        for lbl, da in ds.items():

            def _y_model(x: float, lbl: str = lbl) -> float:
                return binding_1site(
                    x, pars["K"], pars[f"S0_{lbl}"], pars[f"S1_{lbl}"], ds.is_ph
                )

            # Define symbolic closest points using PyMC-compatible operations
            x_prime = pm.Deterministic(
                f"x_prime_{lbl}",
                pm.math.stack(  # noqa: PD013
                    [
                        closest_point_on_curve(
                            lambda x_val: _y_model(x_val).eval(),  # type: ignore[attr-defined]
                            x_obs,
                            y_obs,
                        )
                        for x_obs, y_obs in zip(xc, da.y, strict=True)
                    ]
                ),
            )

            y_prime = pm.Deterministic(
                f"y_prime_{lbl}",
                pm.math.stack([_y_model(x) for x in x_prime.eval()]),  # noqa: PD013
            )
            y_model = pm.Deterministic(
                f"y_model_{lbl}",
                pm.math.stack([_y_model(x) for x in xc]),  # noqa: PD013
            )
            ## TODO:  y_model = as_tensor_variable([_y_model(x) for x in xc])

            mask = as_tensor_variable(da.mask)
            # Orthogonal distance likelihood
            distances = ((x_prime - xc) / (xe_mag * x_errc)) ** 2 + (
                (y_prime - y_model) / (ye_mag * da.y_err)
            ) ** 2
            pm.Normal(
                f"orthogonal_likelihood_{lbl}",
                mu=distances[mask],
                sigma=1,
                observed=np.zeros(len(distances[mask].eval())),
            )
        # Inference
        return pm.sample(n_samples, cores=4, return_inferencedata=True)
    ## TODO:  return process_trace(trace, params.keys(), ds, 0)


# ------------------------------------------------------------------
# Helper: weighted statistics
# ------------------------------------------------------------------


def weighted_stats(
    values: dict[str, list[float]], stderr: dict[str, list[float]]
) -> dict[str, tuple[float, float]]:
    """Weighted mean and stderr for control priors."""
    results: dict[str, tuple[float, float]] = {}
    for sample in values:  # noqa:PLC0206
        x = np.array(values[sample])
        se = np.array(stderr[sample])
        weighted_mean = np.average(x, weights=1 / se**2)
        weighted_stderr = np.sqrt(1 / np.sum(1 / se**2))
        results[sample] = (weighted_mean, weighted_stderr)
    return results


def fit_binding_pymc_multi(  # noqa: PLR0913
    results: dict[str, FitResult[MiniT]],
    scheme: PlateScheme,
    n_sd: float = 5.0,
    n_xerr: float = 1.0,
    ye_scaling: float = 1.0,
    n_samples: int = 2000,
) -> az.InferenceData:
    """Multi-well PyMC with shared K per control group and per-label noise."""
    # FIXME: pytensor.config.floatX = "float32"  # type: ignore[attr-defined]
    ds = next((r.dataset for r in results.values() if r.dataset), None)
    if ds is None:
        msg = "No valid dataset found in results."
        raise ValueError(msg)
    xc = next(iter(ds.values())).xc
    x_errc = next(iter(ds.values())).x_errc * n_xerr
    labels = list(ds.keys())
    values: dict[str, list[float]] = {}
    stderr: dict[str, list[float]] = {}

    for name, wells in scheme.names.items():
        values[name] = [
            v.result.params["K"].value
            for well, v in results.items()
            if v.result and well in wells
        ]
        stderr[name] = [
            v.result.params["K"].stderr
            for well, v in results.items()
            if v.result and well in wells
        ]
    ctr_ks = weighted_stats(values, stderr)

    with pm.Model():
        ye_mag: dict[str, pm.Distribution] = {
            label: pm.HalfNormal(f"ye_mag_{label}", sigma=ye_scaling)
            for label in labels
        }
        x_true = create_x_true(xc, x_errc, n_xerr)

        # Create shared K parameters for each control group
        k_params = {
            control_name: pm.Normal(
                f"K_{control_name}",
                mu=ctr_ks[control_name][0],
                sigma=0.2,  # FIXME: use var
            )
            for control_name in scheme.names
        }
        for key, r in results.items():
            if r.result and r.dataset:
                ds = r.dataset
                # Determine if the well is associated with a control group
                ctr_name = next(
                    (name for name, wells in scheme.names.items() if key in wells), ""
                )
                pars = create_parameter_priors(r.result.params, n_sd, key, ctr_name)
                # Use shared K for control group wells or create a unique K otherwise
                K = k_params[ctr_name] if ctr_name else pars[f"K_{key}"]  # noqa: N806

                for lbl, da in ds.items():
                    y_model = binding_1site(
                        x_true,
                        K,
                        pars[f"S0_{lbl}_{key}"],
                        pars[f"S1_{lbl}_{key}"],
                        ds.is_ph,
                    )
                    pm.Normal(
                        f"y_likelihood_{lbl}_{key}",
                        mu=y_model[da.mask],
                        sigma=ye_mag[lbl] * da.y_err,
                        observed=da.y,
                    )

        trace: az.InferenceData = pm.sample(
            n_samples, target_accept=0.9, return_inferencedata=True
        )

    return trace


def fit_binding_pymc_multi2(  # noqa: PLR0913
    results: dict[str, FitResult[MiniT]],
    scheme: PlateScheme,
    bg_err: dict[int, ArrayF],
    n_sd: float = 5.0,
    n_xerr: float = 1.0,
    # Ponder this: ye_scaling: float = 1.0, # This parameter is no longer needed in the same way
    n_samples: int = 2000,
) -> az.InferenceData:
    """Multi-well PyMC with heteroscedastic noise combining buffer and signal."""
    ds_example = next((r.dataset for r in results.values() if r.dataset), None)
    ds = next((result.dataset for result in results.values() if result.dataset), None)

    if ds_example is None:
        msg = "No valid dataset found in results."
        raise ValueError(msg)
    # Extract common data once
    xc = next(iter(ds_example.values())).xc
    x_errc = next(iter(ds_example.values())).x_errc * n_xerr
    labels = list(ds_example.keys())  # e.g., ['y1', 'y2']
    # --- Pre-calculate weighted stats for K priors (remains the same) ---
    values: dict[str, list[float]] = {}
    stderr: dict[str, list[float]] = {}
    for name, wells in scheme.names.items():
        values[name] = [
            v.result.params["K"].value
            for well, v in results.items()
            if v.result
            and well in wells  # and "K" in v.result.params._params # Check if K exists
        ]
        stderr[name] = [
            v.result.params["K"].stderr
            for well, v in results.items()
            if v.result and well in wells  #  and "K" in v.result.params._params
        ]
    ctr_ks = weighted_stats(values, stderr)
    # MAYBE: Restore logger.info(f"Weighted K stats for control groups: {ctr_ks}")

    with pm.Model():
        # --- Common Priors / Variables for the entire model ---
        x_true = create_x_true(xc, x_errc, n_xerr)
        # Global scaling factors for the signal-dependent noise for each label (band)
        # `sigma` prior for HalfNormal should be set considering the expected scale of your signal values.
        # If your fluorescence values are thousands, a sigma of 10-100 might be reasonable for the scaling factor.
        sigma_signal_scale: dict[str, pm.Distribution] = {
            lbl: pm.HalfNormal(f"sigma_signal_scale_{lbl}", sigma=10.0)
            for lbl in labels
        }
        true_buffer = {
            lbl: pm.Normal(f"true_buffer_{lbl}", mu=0, sigma=bg_err[i])
            for i, lbl in enumerate(labels, start=1)
        }
        variance_buffer_contrib = {i: bg_err[i] ** 2 for i in range(1, len(labels) + 1)}

        # Degrees of freedom for Student's T distribution (for robustness)
        # Can be shared or per-label. Shared is often fine for similar data types.
        # this was for student: nu_common = pm.Gamma("nu_common", alpha=2, beta=0.1)

        # Create shared K parameters for each control group
        k_params = {
            control_name: pm.Normal(
                f"K_{control_name}",
                mu=ctr_ks[control_name][0],
                # if ctr_ks[control_name][0] is not np.nan
                # else 7.0,  # Handle case where no K values found
                sigma=0.2,  # FIXME: consider using ctr_ks[control_name][1] for sigma
                # TODO: sigma=ctr_ks[control_name][1] if ctr_ks[control_name][1] is not np.nan else 0.5, # Default sigma for K if no stderr
            )
            for control_name in scheme.names
        }
        print(k_params)
        # --- Loop through each well (key) and its data ---
        for key, r in results.items():
            if r.result and r.dataset:
                ds = r.dataset
                # Determine if the well is associated with a control group
                ctr_name = next(
                    (name for name, wells in scheme.names.items() if key in wells), ""
                )
                # Parameters for S0, S1 (unique to each well and label)
                # `create_parameter_priors` should return a dict of PyMC distributions for S0, S1 for this key/well
                pars = create_parameter_priors(r.result.params, n_sd, key, ctr_name)
                # Use shared K for control group wells or create a unique K otherwise
                k_param_for_well = k_params[ctr_name] if ctr_name else pars[f"K_{key}"]
                # --- Loop through each fluorescence label (e.g., 'y1', 'y2') within the current well ---
                for i, (lbl, da) in enumerate(ds.items(), start=1):
                    # for lbl, da in ds.items():
                    # --- Predicted signal from the binding model ---
                    y_model_signal = binding_1site(
                        x_true,
                        k_param_for_well,
                        pars[f"S0_{lbl}_{key}"],
                        pars[f"S1_{lbl}_{key}"],
                        ds.is_ph,
                    )
                    # Predicted Total Fluorescence (Signal + Buffer) for the likelihood mu
                    mu_total_pred = pm.Deterministic(
                        f"mu_total_pred_{lbl}_{key}", y_model_signal + true_buffer[lbl]
                    )
                    # --- Model the Noise ---
                    # A common model for fluorescence noise is that standard deviation scales with sqrt(mean)
                    # or that variance scales linearly with mean (similar to Poisson, but continuous/scaled).
                    # Let's use a simpler and common power-law for noise (SD = a * mu^b)
                    # Here, we'll assume `b=0.5` (sqrt) and `a` is our `sigma_signal_scale`.
                    # Calculate the variance for the signal component (excluding buffer noise)
                    # Ensuring non-negativity for sqrt
                    # Variance from signal itself (heteroscedastic: proportional to predicted signal mean)
                    # Use pm_math.maximum to avoid issues with negative predicted signals for variance
                    variance_signal_contrib = sigma_signal_scale[lbl] * pm_math.maximum(
                        1e-6, y_model_signal
                    )
                    # Total variance is the sum of independent variances
                    total_variance_obs = pm.Deterministic(
                        f"total_variance_obs_{lbl}_{key}",
                        # variance_buffer_contrib + variance_signal_contrib,
                        variance_buffer_contrib[i] + variance_signal_contrib,
                    )
                    # Total standard deviation for the likelihood
                    sigma_obs = pm.Deterministic(
                        f"sigma_obs_{lbl}_{key}", pm_math.sqrt(total_variance_obs)
                    )
                    # --- Likelihood ---
                    # Use Student's T distribution for robustness against outliers
                    # Apply mask to observed data and corresponding mu/sigma
                    # This is the learned, heteroscedastic SD
                    pm.Normal(
                        f"y_likelihood_{lbl}_{key}",
                        # this was for student: nu=nu_common,  # Use the shared nu_common
                        mu=mu_total_pred[da.mask],
                        sigma=sigma_obs[da.mask],
                        observed=da.y,
                    )

        trace: az.InferenceData = pm.sample(
            n_samples, target_accept=0.9, return_inferencedata=True
        )

    return trace


# ------------------------------------------------------------------
# 2.3  Posterior-predictive helper - visualise one well at a time
# ------------------------------------------------------------------
def plot_ppc_well(
    trace: az.InferenceData,
    key: str,
    labels: list[str] | None = None,
    figsize: tuple[float, float] = (8, 4),
) -> figure.Figure:
    """Draw posterior predictive samples for a particular well (and all its labels).

    The returned figure can be displayed with matplotlib.

    Parameters
    ----------
    trace   : az.InferenceData
        Trace produced by ``fit_binding_pymc_advanced``.
    key     : str
        Well identifier (e.g. 'A01').
    labels  : list[str] | None
        Names of the bands to show.  If *None* the function will
        automatically look for all variables starting with
        ``'y_'`` that contain this key.
    figsize: tuple[float, float]
        size?

    Return
    ------
    figure.Figure
        Plot
    """
    if labels is None:
        labels = [
            var.split("_")[1]
            for var in trace.posterior.data_vars  # type: ignore[attr-defined]
            if f"{key}" in var and var.startswith("y_")
        ]

    fig, axes = plt.subplots(
        len(labels), 1, figsize=(figsize[0], figsize[1] * len(labels)), sharex=True
    )
    if len(labels) == 1:
        axes = [axes]

    for ax, lbl in zip(axes, labels, strict=True):
        var_name = f"y_{lbl}_{key}"
        az.plot_ppc(  # type: ignore[no-untyped-call]
            az.from_dict(
                {"posterior_predictive": trace.posterior_predictive[var_name]},  # type: ignore[attr-defined]
                coords={"y": [lbl]},
                dims={"y": 0},
            ),
            ax=ax,
        )
        ax.set_title(f"Well {key} - band {lbl}")
        ax.set_xlabel("Observed y")
        ax.set_ylabel("Posterior predictive")

    plt.tight_layout()
    return fig


# ------------------------------------------------------------------
# 2.4  Comparison of posteriors with deterministic fits
# ------------------------------------------------------------------
def compare_posteriors(
    trace: az.InferenceData, results: dict[str, FitResult[MiniT]]
) -> None:
    """Print posterior mean ± 95 % C.I.

    For the K parameter for each well, and juxtapose it with the deterministic K
    (from fit_binding_pymc).

    Parameters
    ----------
    trace   : az.InferenceData
        Output of ``fit_binding_pymc_advanced``.
    results : dict[str, FitResult[MiniT]]
        Deterministic fits produced by the old pipeline.
    """
    # Summarise the trace
    summary = az.summary(trace, var_names=["K_*"], round_to=3)
    print("\nPosterior for K (averaged over all draws)")
    print(summary[["mean", "hdi_2.5%", "hdi_97.5%"]])

    # Add deterministic K to the table for easy comparison
    deterministic = {}
    for k, fr in results.items():
        if fr.result and "K" in fr.result.params:
            deterministic[k] = fr.result.params["K"].value

    print("\nDeterministic K  (fit_binding_pymc)")
    for k, v in deterministic.items():
        print(f"  {k:6s}  {v:0.3f}")

    # Align rows
    table = summary.join(pd.Series(deterministic, name="deterministic_K"))
    print("\nCombined table")
    print(table[["mean", "hdi_2.5%", "hdi_97.5%", "deterministic_K"]])


def _create_pymc_priors(
    params: Parameters, n_sd: float, key: str = "", ctr_name: str = ""
) -> dict[str, pm.Distribution]:
    """Create PyMC prior distributions from lmfit Parameters."""
    priors = {}

    def param_name(p_name: str) -> str:
        return f"{p_name}_{key}" if key else p_name

    for name, p in params.items():
        # Use a small default sigma if stderr is not available
        sigma = max(p.stderr * n_sd, 1e-3) if p.stderr else 1.0
        # Skip creating a separate K prior if it belongs to a control group
        if ctr_name and name == "K":
            continue
        priors[param_name(name)] = pm.Normal(param_name(name), mu=p.value, sigma=sigma)
    return priors


def _create_pymc_x_true(
    xc: ArrayF, x_errc: ArrayF, n_xerr: float
) -> ArrayF | pm.Deterministic:
    """Create latent variables for x-values with uncertainty."""
    if n_xerr > 0 and np.any(x_errc > 0):
        x_err_scaled = x_errc * n_xerr
        xd = -np.diff(xc)
        xd_err = np.sqrt(x_err_scaled[:-1] ** 2 + x_err_scaled[1:] ** 2)
        # Lower bound to prevent unphysical ordering
        lower = xd.min() - 2.5 * xd_err[np.argmin(xd)]

        x_diff = pm.TruncatedNormal("x_diff", mu=xd, sigma=xd_err, lower=lower)
        x_start = pm.Normal("x_start", mu=xc[0], sigma=x_err_scaled[0])
        x_cumsum = pm.math.cumsum(x_diff)
        return pm.Deterministic(
            "x_true", pm.math.concatenate([[x_start], x_start - x_cumsum])
        )
    return xc  # Treat x as fixed if no error is provided


class FitConfig(TypedDict):
    """Store PyMC-fit setup parameters."""

    n_sd: float
    n_xerr: float
    ye_scaling: float
    n_samples: int


FitMethod = Literal["bayesian", "frequentist", "robust"]


class FitConfig2(BaseModel):
    """Store PyMC-fit setup parameters."""

    n_sd: float = 10.0
    n_xerr: float = 1.0
    ye_scaling: float = 1.0
    n_samples: int = 2000
    method: FitMethod = "bayesian"

    model_config = ConfigDict(extra="forbid")


DEFAULT_CONFIG = None


class Fitter:
    """Adapter for all fit."""

    def __init__(
        self, method: FitMethod = "bayesian", config: FitConfig | None = None
    ) -> None:
        self.method = method
        self.config = config or DEFAULT_CONFIG

    def fit(self, dataset: Dataset) -> FitResult[MiniT]:
        """Unified fitting interface."""
        if self.method == "bayesian":
            return self._fit_bayesian(dataset)
        if self.method == "frequentist":
            return self._fit_frequentist(dataset)
        return FitResult()

    def _fit_bayesian(self, dataset: Dataset) -> FitResult[MiniT]:
        print(dataset)
        return FitResult()

    def _fit_frequentist(self, dataset: Dataset) -> FitResult[MiniT]:
        print(dataset)
        return FitResult()


def fit_pymc_hierarchical(  # noqa: PLR0913
    results: dict[str, FitResult[MiniT]],
    scheme: PlateScheme,
    bg_err: dict[int, ArrayF],
    n_sd: float = 5.0,
    n_xerr: float = 1.0,
    n_samples: int = 2000,
) -> az.InferenceData:
    """
    Analyze multiple titrations with a hierarchical Bayesian model.

    This model shares information about the dissociation constant 'K' among
    wells belonging to the same control group, leading to more robust estimates.

    Parameters
    ----------
    results : dict[str, FitResult[MiniT]]
        A dictionary mapping well IDs to their initial `FitResult` from a
        prior `fit_lm` run.
    scheme : PlateScheme
        The plate scheme defining control groups.
    bg_err : dict[int, ArrayF]
        Background error for each signal band.
    n_sd : float
        The number of standard deviations for the prior width of S0/S1.
    n_xerr : float
        Scaling factor for x-value uncertainties.
    n_samples : int
        Number of MCMC samples.

    Returns
    -------
    az.InferenceData
        The PyMC trace containing the posterior distributions.

    Raises
    ------
    ValueError
        With invalid dataset.
    """
    ds_template = next(r.dataset for r in results.values() if r.dataset)
    if not ds_template:
        msg = "No valid dataset found in results."
        raise ValueError(msg)

    xc = next(iter(ds_template.values())).xc
    x_errc = next(iter(ds_template.values())).x_errc * n_xerr
    labels = list(ds_template.keys())

    # --- Pre-calculate weighted stats for K priors ---
    k_values = {
        name: [
            r.result.params["K"].value
            for well, r in results.items()
            if r.result and well in wells
        ]
        for name, wells in scheme.names.items()
    }
    k_stderr = {
        name: [
            r.result.params["K"].stderr
            for well, r in results.items()
            if r.result and r.result.params["K"].stderr and well in wells
        ]
        for name, wells in scheme.names.items()
    }
    print(k_stderr)

    with pm.Model():
        x_true = _create_pymc_x_true(xc, x_errc, n_xerr)

        # --- Priors for noise model ---
        sigma_signal_scale = {
            lbl: pm.HalfNormal(f"sigma_signal_scale_{lbl}", sigma=10.0)
            for lbl in labels
        }
        true_buffer_mean = {
            lbl: pm.Normal(f"true_buffer_{lbl}", mu=0, sigma=bg_err[i + 1])
            for i, lbl in enumerate(labels)
        }
        variance_buffer_contrib = {
            lbl: bg_err[i + 1] ** 2 for i, lbl in enumerate(labels)
        }

        # --- Priors for shared K values in control groups ---
        k_params = {}
        for name in scheme.names:
            mean_k = np.mean(k_values[name]) if k_values[name] else 7.0
            k_params[name] = pm.Normal(f"K_{name}", mu=mean_k, sigma=0.5)

        # --- Loop through each well to define its model ---
        for key, r in results.items():
            if r.result and r.dataset:
                ctr_name = next(
                    (name for name, wells in scheme.names.items() if key in wells), ""
                )
                priors = _create_pymc_priors(r.result.params, n_sd, key, ctr_name)
                k_param = k_params[ctr_name] if ctr_name else priors[f"K_{key}"]

                for lbl, da in r.dataset.items():
                    y_model_signal = binding_1site(
                        x_true,
                        k_param,
                        priors[f"S0_{lbl}_{key}"],
                        priors[f"S1_{lbl}_{key}"],
                        r.dataset.is_ph,
                    )
                    mu_total = pm.Deterministic(
                        f"mu_total_{lbl}_{key}", y_model_signal + true_buffer_mean[lbl]
                    )
                    var_signal = sigma_signal_scale[lbl] * pm_math.maximum(
                        1e-6, y_model_signal
                    )
                    total_var = pm.Deterministic(
                        f"total_var_{lbl}_{key}",
                        variance_buffer_contrib[lbl] + var_signal,
                    )
                    sigma_obs = pm.Deterministic(
                        f"sigma_obs_{lbl}_{key}", pm_math.sqrt(total_var)
                    )

                    pm.Normal(
                        f"y_likelihood_{lbl}_{key}",
                        mu=mu_total[da.mask],
                        sigma=sigma_obs[da.mask],
                        observed=da.y,
                    )

        trace: az.InferenceData = pm.sample(
            n_samples, tune=n_samples // 2, target_accept=0.9, return_inferencedata=True
        )
    return trace
